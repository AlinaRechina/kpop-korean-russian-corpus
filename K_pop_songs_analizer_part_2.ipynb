{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6xR9bka-iNdl",
   "metadata": {
    "id": "6xR9bka-iNdl"
   },
   "source": [
    "### Установка библиотек и модулей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ko5ARgBQTtqk",
   "metadata": {
    "id": "ko5ARgBQTtqk"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed182f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install korean_romanizer\n",
    "from korean_romanizer.romanizer import Romanizer\n",
    "\n",
    "!pip install konlpy\n",
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d03f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "!wget http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
    "!tar -xvf mystem-3.0-linux3.1-64bit.tar.gz\n",
    "!cp mystem /root/.local/bin/mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d47b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk==3.6.6\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import wordnet \n",
    "nltk_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2e6078",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install eng-to-ipa\n",
    "import eng_to_ipa as eti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VJrCF9pjKT17",
   "metadata": {
    "id": "VJrCF9pjKT17"
   },
   "outputs": [],
   "source": [
    "hangul_string = ''\n",
    "with open('hangul without readings.txt', 'r', encoding = 'utf-8') as han_f:\n",
    "    han_t = han_f.readlines()\n",
    "    for ht in han_t:\n",
    "        hangul_string += ht.strip('\\n')\n",
    "\n",
    "hangul = set(hangul_string) # множество корейских символов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VJArMi-6TuUp",
   "metadata": {
    "id": "VJArMi-6TuUp"
   },
   "source": [
    "#### Открываем .json файл и считываем текст песни"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b-ORbudQTsiy",
   "metadata": {
    "id": "b-ORbudQTsiy"
   },
   "outputs": [],
   "source": [
    "# %TEXT% - название нужного .json файла\n",
    "file_name = '%TEXT%.json'\n",
    "with open(file_name, 'r', encoding='utf-8') as f:\n",
    "    song_meta_lyrics = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd0be0f",
   "metadata": {
    "id": "7bd0be0f"
   },
   "source": [
    "### Токенизация, определение языка, транслитерация, перевод тегов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eZxoxFDszowZ",
   "metadata": {
    "id": "eZxoxFDszowZ"
   },
   "outputs": [],
   "source": [
    "from eng_to_ipa.rhymes import get_rhymes\n",
    "# ПЕРЕВОД\n",
    "\n",
    "light_dict_name = 'kor_rus_dict_light.json'\n",
    "heavy_dict_name = 'kor_rus_dict_heavy.json'\n",
    "\n",
    "ld_f = open(light_dict_name, 'r', encoding='utf-8')\n",
    "ld = json.load(ld_f)\n",
    "\n",
    "hd_f = open(heavy_dict_name, 'r', encoding='utf-8')\n",
    "hd = json.load(hd_f)\n",
    "\n",
    "\n",
    "def definer(word, gr, light_dict, heavy_dict):\n",
    "    # в словаре для экономии времени будут искаться только\n",
    "    # корни, а не вообще любые морфемы\n",
    "    verb_tags = ['V', 'A', 'V, aux', 'A, aux', 'V, cop', 'V, cop, neg', \n",
    "                 'V, ger', 'V, partcp', 'A, partcp']\n",
    "    non_verb_tags = ['S', 'S, aux', 'QUANT', 'NUM', 'ANUM', 'ADV', \n",
    "                     'INTJ', 'PRO']\n",
    "    translation = ''\n",
    "    if gr in verb_tags:\n",
    "        light_translation = light_dict.get(word+'다', ['перевод не найден'])\n",
    "        for tr in light_translation:\n",
    "            translation += tr +'; '\n",
    "        translation = translation.strip('; ')\n",
    "        if translation == 'перевод не найден':\n",
    "            translation = heavy_dict.get(word+'다', 'перевод не найден')\n",
    "    elif gr in non_verb_tags:\n",
    "        light_translation = light_dict.get(word, ['перевод не найден'])\n",
    "        for tr in light_translation:\n",
    "            translation += tr +'; '\n",
    "        translation = translation.strip('; ')\n",
    "        if translation == 'перевод не найден':\n",
    "            translation = heavy_dict.get(word, 'перевод не найден')\n",
    "    \n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seKOtZUhipxf",
   "metadata": {
    "id": "seKOtZUhipxf"
   },
   "outputs": [],
   "source": [
    "# РАЗБОР KKMA / NLTK\n",
    "\n",
    "def analizer(s):\n",
    "    token_list = []\n",
    "    tokens = kkma.pos(s)\n",
    "    for t in tokens:\n",
    "        if t[1] == 'OL':\n",
    "            lang = 1\n",
    "            transcr = s\n",
    "            gr = nltk.pos_tag([s])[0][1]\n",
    "            return [{'lex':s, 'gr':gr, 'lang':1}]\n",
    "        # а вообще надо сохранять знаки препинания?\n",
    "        # я оставила на всякий случай, но избавиться\n",
    "        # от них очень легко\n",
    "        elif t[1][0] == 'S':\n",
    "            lang = 'none'\n",
    "            transcr = t[0]\n",
    "            gr = 'punct'\n",
    "        else:\n",
    "            lang = 0\n",
    "            gr = t[1]\n",
    "        \n",
    "        token = t[0] # тут можно добавить окончания к предикатам!\n",
    "        token_list.append({'lex':token, 'gr':gr, 'lang':lang})\n",
    "        \n",
    "        \n",
    "    return(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "un1Um5FoXA77",
   "metadata": {
    "id": "un1Um5FoXA77"
   },
   "outputs": [],
   "source": [
    "# ТРАНСЛИТЕРАЦИЯ\n",
    "\n",
    "def transl2(han):\n",
    "    try:\n",
    "        transcr = Romanizer(han)\n",
    "        transcr = transcr.romanize()\n",
    "    except Exception:\n",
    "        transcr = 'transcription_error'\n",
    "    return transcr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ssqyf1_4C9Nt",
   "metadata": {
    "id": "ssqyf1_4C9Nt"
   },
   "outputs": [],
   "source": [
    "# РАЗДЕЛЕНИЕ СУЩЕСТВИТЕЛЬНЫХ И ЧАСТИЦ\n",
    "\n",
    "def nj_separator(m_list):\n",
    "    # NN_ + JK_ + JK = максимально длинный паттерн\n",
    "    cut = []\n",
    "    just_particles = [\"JK\", \"JKM\", \"JX\", \"JC\"]\n",
    "    for ml in reversed(m_list):\n",
    "        sq = []\n",
    "        if ml['gr'] in just_particles:\n",
    "            sq = [ml['lex'], 'PART', 0, ml['lex']]\n",
    "        elif ml['gr'] == 'JKC':\n",
    "            sq = [ml['lex'], 'PART, nom', 0, ml['lex']]\n",
    "        elif ml['gr'] == 'JKG':\n",
    "            sq = [ml['lex'], 'PART, gen', 0, ml['lex']]\n",
    "        elif ml['gr'] == 'JKO':\n",
    "            sq = [ml['lex'], 'PART, acc', 0, ml['lex']]\n",
    "        elif ml['gr'] == 'NNP' or ml['gr'] == 'NNG':\n",
    "            sq = [ml['lex'], 'S', 0, ml['lex']]\n",
    "        else:\n",
    "            sq = [ml['lex'], 'PART', 0, ml['lex']]\n",
    "        \n",
    "        cut.insert(0, sq)\n",
    "\n",
    "    return cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CNY7h8Ffpiqm",
   "metadata": {
    "id": "CNY7h8Ffpiqm"
   },
   "outputs": [],
   "source": [
    "# ПЕРЕВОД ТЕГОВ KKMA В СИСТЕМУ НКРЯ, ТРАНСЛИТЕРАЦИЯ И ПЕРЕВОД\n",
    "# fw - словоформа от пробела до пробела\n",
    "# ms - [{'lex':морфема, 'gr':gr, 'lang':0, \n",
    "#        'transl':перевод, 'transcr':транслитерация}]\n",
    "\n",
    "def ncrl(fw, ms):\n",
    "    final_tag = ''\n",
    "    lex = fw\n",
    "\n",
    "    # сначала выпишем все теги, присвоенные kkma\n",
    "    tags = []\n",
    "    mors = []\n",
    "\n",
    "    # оставляем целое слово и его целый разбор\n",
    "    full_word = fw\n",
    "    punctuation_tags = ['SF','SE','SS','SP','SO',\n",
    "                        'SW', 'ON'] # если какие-то не слова всё же попали\n",
    "    \n",
    "    for m in ms:\n",
    "        if m['lang'] == 0:\n",
    "            tags.append(m['gr'])\n",
    "            mors.append(m['lex'])\n",
    "        elif m['lang'] == 1:\n",
    "            return en_ncrl(ms, full_word)\n",
    "        else:\n",
    "            final_tag = (m['gr'])\n",
    "            return [[full_word, final_tag, 3, full_word]]\n",
    "    \n",
    "    if len(mors)>0:\n",
    "        m_lex = mors[0]\n",
    "    else:\n",
    "        m_lex = fw\n",
    "\n",
    "    # проверяем наличие морфем,\n",
    "    # приписывающих часть речи\n",
    "\n",
    "    # деепричастия\n",
    "    if 'EFI' in tags or 'ECE' in tags or 'ECD' in tags and ms[-1]['lex']!='게':\n",
    "        if 'VA' in tags or 'VXA' in tags:\n",
    "            final_tag = 'A, ger'\n",
    "        elif 'VV' in tags or 'VXV' in tags or 'VCP' in tags or 'VCN' in tags:\n",
    "            final_tag = 'V, ger'\n",
    "        return [[full_word, final_tag, 0, m_lex]]\n",
    "    \n",
    "    # причастия\n",
    "    if 'ETD' in tags:\n",
    "        if 'VA' in tags:\n",
    "            final_tag = 'A, partcp'\n",
    "        else:\n",
    "            final_tag = 'V, partcp'\n",
    "        return [[full_word, final_tag, 0, m_lex]]\n",
    "\n",
    "    # номинализация, вербализация и аджективизация\n",
    "    if 'ETN' in tags:\n",
    "        final_tag = 'S'\n",
    "        return [[full_word, final_tag, 0, m_lex]]\n",
    "    \n",
    "    if 'XSN' in tags:\n",
    "        final_tag = 'S'\n",
    "        return [[full_word, final_tag, 0, m_lex]]\n",
    "\n",
    "    if 'XSV' in tags:\n",
    "        final_tag = 'V'\n",
    "        return [[full_word, final_tag, 0, m_lex]]\n",
    "\n",
    "    if 'XSA' in tags:\n",
    "        final_tag = 'A'\n",
    "        return [[full_word, final_tag, 0, m_lex]]\n",
    "\n",
    "    # отглагольные деепричастия\n",
    "    if 'ECD' in tags and ms[-1]['lex']=='게':\n",
    "        final_tag = 'ADV'\n",
    "        return [[full_word, final_tag, 0, lex]]\n",
    "    \n",
    "    # отделение существительных от частиц и падежей\n",
    "    particles = ['JX', 'JKS', 'JKC', 'JKG', 'JKO', 'JKM', 'JC']\n",
    "    for pt in particles:\n",
    "        if pt in tags and 'NNG' in tags or pt in tags and 'NNP' in tags:\n",
    "            return nj_separator(ms)\n",
    "\n",
    "    # приписывание всех остальных тегов\n",
    "    if 'VV' in tags:\n",
    "        final_tag = 'V'\n",
    "        return [[full_word, final_tag, 0, m_lex]]\n",
    "    \n",
    "    if 'VA' in tags or 'MDT' in tags:\n",
    "        final_tag = 'A'\n",
    "        return [[full_word, final_tag, 0, m_lex]]\n",
    "\n",
    "    if 'NNP' in tags or 'NNG' in tags:\n",
    "        final_tag = 'S'\n",
    "        return [[full_word, final_tag, 0, lex]]\n",
    "    \n",
    "    if 'NNB' in tags or 'NNM' in tags:\n",
    "        final_tag = 'S, aux'\n",
    "        return [[full_word, final_tag, 0, lex]]\n",
    "    \n",
    "    if 'NR' in tags:\n",
    "        final_tag = 'NUM'\n",
    "        return [[full_word, final_tag, 0, lex]]\n",
    "    \n",
    "    if 'NP' in tags:\n",
    "        final_tag = 'PRO'\n",
    "        return [[full_word, final_tag, 0, lex]]\n",
    "\n",
    "    if 'VXV' in tags:\n",
    "        final_tag = 'V, aux'\n",
    "        return [[full_word, final_tag, 0, m_lex]]\n",
    "\n",
    "    if 'VXA' in tags:\n",
    "        final_tag = 'A, aux'\n",
    "        return [[full_word, final_tag, 0, lex]]\n",
    "    \n",
    "    if 'VCP' in tags:\n",
    "        final_tag = 'V, cop'\n",
    "        return [[full_word, final_tag, 0, lex]]\n",
    "    \n",
    "    if 'VCN' in tags:\n",
    "        final_tag = 'V, cop, neg'\n",
    "        return [[full_word, final_tag, 0, lex]]\n",
    "    \n",
    "    if 'MDN' in tags:\n",
    "        final_tag = 'ANUM'\n",
    "        return [[full_word, final_tag, 0, lex]]\n",
    "    \n",
    "    if 'MAG' in tags or 'MAC' in tags:\n",
    "        final_tag = 'ADV'\n",
    "        return [[full_word, final_tag, 0, lex]]\n",
    "\n",
    "    if 'IC' in tags:\n",
    "        final_tag = 'INTJ'\n",
    "        return [[full_word, final_tag, 0, lex]]\n",
    "\n",
    "    return [[full_word, final_tag, 3, lex]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cHEkqWh2SLxM",
   "metadata": {
    "id": "cHEkqWh2SLxM"
   },
   "outputs": [],
   "source": [
    "# эта функция разлепляет contracted forms\n",
    "def uncontract(contracted):\n",
    "    forms = []\n",
    "    annots = {'t':['not','ADV',1], 've':['have','V',1], 'll':['will', 'MD',1],\n",
    "              'd':['would', 'MD',1], 're':['are', 'V',1], 'm':['am', 'V',1]}\n",
    "    pros = ['he', 'she', 'it', 'there', 'here', 'where', 'who', 'that']\n",
    "\n",
    "    parts = contracted.split('\\'')\n",
    "\n",
    "    if contracted == 'let\\'s':\n",
    "        return [['let','V', 1],['us', 'PRO', 1]]\n",
    "\n",
    "    # mustn't've, couldn't've ect\n",
    "    if len(parts) == 3 and parts[-1] == 've':\n",
    "        forms = uncontract(parts[:-1])\n",
    "        forms.append(annots['ve'])\n",
    "        return forms\n",
    "        \n",
    "    # n't\n",
    "    elif parts[-1] == 't':\n",
    "        true_first = parts[0][:-1]\n",
    "        forms.append([true_first, nltk.pos_tag([true_first])[0], 1])\n",
    "        forms.append(annots['t'])\n",
    "        return forms\n",
    "\n",
    "    # 's - генитив или is (has)\n",
    "    elif parts[-1] == 's':\n",
    "        if parts[0] in pros:\n",
    "            forms.append([parts[0], nltk.pos_tag([parts[0]])[0], 1])\n",
    "            forms.append(['is', 'V', 1])\n",
    "            return  forms\n",
    "        else:\n",
    "            return [[contracted, 'S, sg, gen', 1]]\n",
    "\n",
    "    # s' - генитив  множественного \n",
    "    elif parts[-1] == '' and parts[-2][-1] == 's':\n",
    "        return [[contracted, 'S, pl, gen', 1]]\n",
    "\n",
    "    else:\n",
    "        try: \n",
    "            forms.append([parts[0], nltk.pos_tag([parts[0]])[0], 1])\n",
    "            forms.append(annots[parts[-1]])\n",
    "            return forms\n",
    "        except:\n",
    "            return [[contracted, nltk.pos_tag(contracted)[0], 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X3hVnDUoeXl8",
   "metadata": {
    "id": "X3hVnDUoeXl8"
   },
   "outputs": [],
   "source": [
    "# ПЕРЕВОД ТЕГОВ nltk В СИСТЕМУ НКРЯ <-\n",
    "# ems - {'lex':token, 'gr':gr, 'transcr':transcr, 'lang':1}\n",
    "\n",
    "def en_ncrl(ems, full_word):\n",
    "    etags = []\n",
    "\n",
    "    for em in ems:\n",
    "        etags.append(em['gr'])\n",
    "    \n",
    "    # боремся с проблемами с contructed forms, которые возникают из-за\n",
    "    # отсутствия контекста: nltk начинает их парсить как NN все подряд\n",
    "    if '\\'' in em['lex']:\n",
    "        return(uncontract(full_word))\n",
    "\n",
    "    final_tag = '' # на случай, если что-то пошло не так\n",
    "\n",
    "    # переводим в НКРЯ, если это не была contructed form\n",
    "    if 'FW' in etags:\n",
    "        final_tag = 'NONLEX'\n",
    "    if 'UH' in etags:\n",
    "        final_tag = 'INTJ'\n",
    "    if 'IN' in etags:\n",
    "        final_tag = 'PR'\n",
    "    if 'CC' in etags:\n",
    "        final_tag = 'CONJ'\n",
    "    if 'PDT' in etags:\n",
    "        final_tag = 'ANUM'\n",
    "    if 'RP' in etags:\n",
    "        final_tag = 'PART'\n",
    "    if 'DT' in etags:\n",
    "        final_tag = 'ART'\n",
    "    if 'LS' in etags:\n",
    "        final_tag = 'LS'\n",
    "    if 'TO' in etags:\n",
    "        final_tag = 'PART'\n",
    "    if 'WDT' in etags:\n",
    "        final_tag = 'APRO'\n",
    "    if 'WP' in etags or 'PRP' in etags:\n",
    "        final_tag = 'PRO'\n",
    "    if 'WRB' in etags:\n",
    "        final_tag = 'ADVPRO'\n",
    "    if 'PRPS' in etags:\n",
    "        final_tag = 'APRO'\n",
    "    if 'CD' in etags:\n",
    "        final_tag = 'NUM'\n",
    "    if 'EX' in etags or 'RB' in etags:\n",
    "        final_tag = 'ADV'\n",
    "    if 'RBR' in etags:\n",
    "        final_tag = 'ADV, comp'\n",
    "    if 'RBS' in etags:\n",
    "        final_tag = 'ADV, supr'\n",
    "    if 'JJ' in etags:\n",
    "        final_tag = 'ADJ'\n",
    "    if 'JJR' in etags:\n",
    "        final_tag = 'ADJ, comp'\n",
    "    if 'JJS' in etags:\n",
    "        final_tag = 'ADJ, supr'\n",
    "    if 'NN' in etags or 'NNP' in etags:\n",
    "        final_tag = 'S, sg'\n",
    "    if 'NNS' in etags or 'NNPS' in etags:\n",
    "        final_tag = 'S, pl'\n",
    "    if 'VB' in etags:\n",
    "        final_tag = 'V'\n",
    "    if 'VBG' in etags:\n",
    "        final_tag = 'V, ger'\n",
    "    if 'VBD' in etags or 'VBP' in etags:\n",
    "        final_tag = 'V, praet'\n",
    "    if 'VBN' in etags:\n",
    "        final_tag = 'V, partcp'\n",
    "    if 'VBZ' in etags:\n",
    "        final_tag = 'V, praet, 3p, sg'\n",
    "    if 'MD' in etags:\n",
    "        final_tag = 'V'\n",
    "    if 'POS' in etags:\n",
    "        final_tag = 'S, gen' # но по идее эти формы отсекаются раньше\n",
    "\n",
    "    return [[full_word, final_tag, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8-dBAB_S4kNb",
   "metadata": {
    "id": "8-dBAB_S4kNb"
   },
   "outputs": [],
   "source": [
    "# ТОКЕНИЗАЦИЯ\n",
    "\n",
    "def tag_ana(org, ld, hd):\n",
    "    anas = []\n",
    "    ncrl_token_list = []\n",
    "    toks = org.split(' ')\n",
    "    i_char = 0 # счётчик символов\n",
    "    i_wn = 0 # счётчик слов\n",
    "\n",
    "    for tok in toks:\n",
    "        end_of_the_token = i_char + len(tok) - 1\n",
    "        tok = tok.strip('\\.\\,\\:\\'\\\"\\-\\+\\=\\$\\%\\#\\@\\/\\\\\\[\\]\\{\\}\\(\\)\\*\\&\\^\\`\\~\\n')\n",
    "        done_tok = analizer(tok) # просто обработка kkma или nltk\n",
    "        words = ncrl(tok, done_tok) # объединение морфем + теги нкря\n",
    "        # in words each w = [full_form, ncr_tag, lang, (lex - только для кор)]\n",
    "        \n",
    "        for w in words:\n",
    "            small_ana = {'pos':w[1], 'lang':w[2]}\n",
    "            if small_ana['lang'] == 0:\n",
    "                small_ana['lex'] = w[3]\n",
    "                small_ana['transcr'] = transl2(w[0])\n",
    "                translation = definer(w[3], w[1], ld, hd)\n",
    "                if translation != '':\n",
    "                      small_ana['trans_ru'] = translation\n",
    "            elif small_ana['lang'] == 1:\n",
    "                small_ana['lex'] = nltk_lemmatizer.lemmatize(w[0])\n",
    "\n",
    "            big_ana = {'wf':w[0].lower(), 'wtype':'word', 'ana':small_ana, \n",
    "                       'sentence_index':i_wn, 'off_start':i_char,\n",
    "                       'off_end':end_of_the_token}\n",
    "            if i_wn != len(toks) - 1:\n",
    "                big_ana['next_word'] = i_wn + 1\n",
    "\n",
    "            anas.append(big_ana)\n",
    "            i_wn += 1\n",
    "\n",
    "        i_char = end_of_the_token + 2 # пробел\n",
    "\n",
    "    for a in anas:\n",
    "        a['sentence_index_neg'] = len(anas) - a['sentence_index']\n",
    "        \n",
    "    return anas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ktru55gh8R78",
   "metadata": {
    "id": "Ktru55gh8R78"
   },
   "source": [
    "### Обработка русского текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jVwudD9u8OrB",
   "metadata": {
    "id": "jVwudD9u8OrB"
   },
   "outputs": [],
   "source": [
    "def rus_tagger(phrase, para_alignment):\n",
    "  # заполняем meta пустыми значениями, т.к. для русского не актуально\n",
    "    para_alignment['off_start'] = 0\n",
    "    para_alignment['off_end'] = len(phrase) - 1\n",
    "    ready = {'text':phrase, 'lang':2, 'para_alignment':para_alignment,\n",
    "             'meta':{'possibly_rhymed_with':'', 'rhymed_with':'',\n",
    "                     'last_word':'', 'ending':'', 'last_vowel':'',\n",
    "                     'length_in_syllables':0}}\n",
    "\n",
    "    words = []\n",
    "    char_i = 0\n",
    "    word_i = 0\n",
    "    \n",
    "    \n",
    "    punct = '\\.\\,\\:\\'\\\"\\-\\+\\=\\$\\%\\#\\@\\/\\\\\\[\\]\\{\\}\\(\\)\\*\\&\\^\\`\\~\\n '\n",
    "    phrase = phrase.strip(punct)\n",
    "    mb_ana = m.analyze(phrase)\n",
    "    m_ana = []\n",
    "    # удалим все пробелы из анализа\n",
    "    for mb in mb_ana:\n",
    "        if mb['text'] != ' ':\n",
    "            m_ana.append(mb)\n",
    "\n",
    "    for m_word in m_ana[:-1]:\n",
    "        if 'analysis' in m_word and len(m_word['analysis'])>0:\n",
    "            gr = m_word['analysis'][0]['gr']\n",
    "            pos = gr.split('=')[0].split(',')[0]\n",
    "            lex = m_word['analysis'][0]['lex']\n",
    "            ana = [{'pos':pos, 'lex':lex}]\n",
    "            word = {'wf':m_word['text'], 'sentence_index_neg':len(m_ana)-word_i,\n",
    "                     'sentence_index':word_i, 'ana':ana, 'wtype':'word',}\n",
    "                \n",
    "        elif 'analysis' in m_word and len(m_word['analysis']) == 0:\n",
    "            word = {'wf':m_word['text'], 'wtype':'word', \n",
    "                    'sentence_index':word_i,\n",
    "                    'sentence_index_neg':len(m_ana)-word_i}\n",
    "           \n",
    "        elif 'analysis' not in m_word:\n",
    "            word = {'wf':m_word['text'], 'wtype':'punct',\n",
    "                    'sentence_index':word_i,\n",
    "                    'sentence_index_neg':len(m_ana)-word_i}\n",
    "            \n",
    "        word_i += 1\n",
    "        if word_i < len(m_ana):\n",
    "                word['next_word'] = word_i\n",
    "        \n",
    "        word['off_start'] = char_i\n",
    "        word['off_end'] = char_i + len(m_word['text']) - 1\n",
    "        char_i += + len(m_word['text']) + 1 # помним про пробелы\n",
    "        \n",
    "        words.append(word)\n",
    "\n",
    "    ready['words'] = words\n",
    "    return ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yndSucxzB4DB",
   "metadata": {
    "id": "yndSucxzB4DB"
   },
   "source": [
    "### Стиховедческая разметка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48i0R767D-qF",
   "metadata": {
    "id": "48i0R767D-qF"
   },
   "source": [
    "#### Рифма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Bq9eLjKzP0hw",
   "metadata": {
    "id": "Bq9eLjKzP0hw"
   },
   "outputs": [],
   "source": [
    "# ТРАНСЛИТЕРАЦИЯ КОРЕЙСКОГО -> МФА\n",
    "def rom_to_ipa(rom):\n",
    "    \n",
    "    # переводим диграфы согласных в МФА\n",
    "    rom = rom.replace('ng', 'ŋ')\n",
    "    rom = rom.replace('ch', 'ʧʰ')\n",
    "    rom = rom.replace('j', 'ʤ')\n",
    "\n",
    "    vowels = 'euioa'\n",
    "    voiced = 'mnlŋ'\n",
    "    unvoiced = 'ktpʧ'\n",
    "    voiced_paired = 'gdbʤ'\n",
    "    palatalized = 'gkdtbplmrnh'\n",
    "    pairs = {'g':'k', 'd':'t', 'b':'p', 'ʤ':'ʧ'}\n",
    "    \n",
    "    rom = ' ' + rom + ' '\n",
    "    new_rom = ''\n",
    "\n",
    "    for i in range(1,len(rom)-1):\n",
    "        # придыхательные согласные\n",
    "        if rom[i] in unvoiced and rom[i-1] != rom[i] and rom[i+1] != rom[i]:\n",
    "            new_rom += rom[i] + 'ʰ'\n",
    "\n",
    "        # палатализация\n",
    "        elif rom[i] == 'y':\n",
    "            if rom[i-1] == ' ' or rom[i-1] in vowels:\n",
    "                new_rom += 'y'\n",
    "            elif rom[i-1] in palatalized:\n",
    "                new_rom += 'ʲ' \n",
    "\n",
    "        # обычные согласные: \n",
    "        # интервокальная позиция и после звонких -> звонкий\n",
    "        # начало слова, после глухих, конец слова -> глухой\n",
    "        elif rom[i] in voiced_paired:\n",
    "            if rom[i-1] in vowels and rom[i+1] in vowels:\n",
    "                new_rom += rom[i]\n",
    "            elif rom[i-1] in voiced and rom[i+1] in vowels:\n",
    "                new_rom += rom[i]\n",
    "            else:\n",
    "                new_rom += pairs[rom[i]]\n",
    "\n",
    "        # остальное просто перезаписываем\n",
    "        else:\n",
    "            new_rom += rom[i]\n",
    "\n",
    "    # в современном корейском 애 и 에 совпали в ɛ\n",
    "    new_rom = new_rom.replace('ae', 'ɛ')\n",
    "    new_rom = new_rom.replace('oe', 'wɛ')\n",
    "    new_rom = new_rom.replace('eu', 'ɯ')\n",
    "    new_rom = new_rom.replace(' ui', ' ɰi')\n",
    "    new_rom = new_rom.replace('ui', 'i')\n",
    "    new_rom = new_rom.replace('eo', 'ʌ')\n",
    "    new_rom = new_rom.replace('a', 'ɑ')\n",
    "    new_rom = new_rom.replace('e', 'ɛ')\n",
    "\n",
    "    # ui\n",
    "    \n",
    "    return new_rom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IDuEB_pHOfWt",
   "metadata": {
    "id": "IDuEB_pHOfWt"
   },
   "outputs": [],
   "source": [
    "# ВЫДЕЛЕНИЕ ПОСЛЕДНЕГО СЛОГА И ПОСЛЕДНЕГО ГЛАСНОГО\n",
    "\n",
    "def ender(transcr):\n",
    "    vowels = 'ɛieuoaɐɤʌɯɔæɶɑɒəʊɜɪ:'\n",
    "    ending = ''\n",
    "    last_vowel = ''\n",
    "    written = 0\n",
    "    for i in range(len(transcr)-1, -1, -1):\n",
    "        if transcr[i] in vowels and written == 0:\n",
    "            last_vowel = transcr[i] + last_vowel\n",
    "            ending = transcr[i] + ending\n",
    "            written = 1\n",
    "        elif transcr[i] in vowels and written == 1:\n",
    "            last_vowel = transcr[i] + last_vowel\n",
    "            ending = transcr[i] + ending\n",
    "        elif transcr[i] in vowels and written == 2:\n",
    "            written = 3\n",
    "        elif transcr[i] not in vowels and written == 0:\n",
    "            ending = transcr[i] + ending\n",
    "        elif transcr[i] not in vowels and transcr[i] != 'ʰ' and transcr[i] != 'ʲ' and written == 1:\n",
    "            ending = transcr[i] + ending\n",
    "            written = 2\n",
    "        elif transcr[i] == 'ʰ' and written < 2:\n",
    "            ending = 'ʰ' + ending\n",
    "        elif transcr[i] == 'ʲ' and written < 2:\n",
    "            ending = 'ʲ' + ending\n",
    "    return last_vowel, ending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6PaxYHKEEWuk",
   "metadata": {
    "id": "6PaxYHKEEWuk"
   },
   "outputs": [],
   "source": [
    "# ДЛЯ POSSIBLY RHYMED\n",
    "\n",
    "rhymed_vowels = {'i':'ɯɪi', 'ɯ':'ɯɪi', 'ɪ':'ɪɯi', \n",
    "                 'u':'uʊ', 'ʊ':'uʊ', 'ə':'ɐ',\n",
    "                 'e':'eæɛ', 'æ':'eæɛ', 'ɛ':'eæɛ',\n",
    "                 'a':'aɐɑ', 'ɐ':'aɐɑə', 'ɑ':'aɐɑ',\n",
    "                 'o':'oɤɔʌ', 'ɤ':'ʌoɤɔ', 'ɔ':'ʌoɤɔ', 'ʌ':'ʌoɔɤ'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xe42dJzlttJM",
   "metadata": {
    "id": "Xe42dJzlttJM"
   },
   "outputs": [],
   "source": [
    "# НАХОЖДЕНИЕ ЗАРИФМОВАННЫХ СТРОК\n",
    "\n",
    "strings = defaultdict(list) \n",
    "# strings = {параграф:[номер_стоки, последний_гласный, \n",
    "#                      последний_слог, последнее слово]}\n",
    "\n",
    "\n",
    "line_keys = song_meta_lyrics[1].keys()\n",
    "\n",
    "for lk in line_keys:\n",
    "    punct = ' ,.\\\"\\'[]()!?-%:;'\n",
    "    last_word = song_meta_lyrics[1][lk][1].split(' ')[-1].strip(punct).lower()\n",
    "    lat_letters = 'qwertyuioplkjhgfdsazxcvbnm'\n",
    "    if len(last_word) != 0:\n",
    "        if last_word[0].lower() in lat_letters:\n",
    "            last_syllable, last_vowel = ender(eti.convert(last_word).strip('*'))\n",
    "        elif last_word[0] in hangul:\n",
    "            last_syllable, last_vowel = ender(\n",
    "                rom_to_ipa(Romanizer(last_word).romanize()))\n",
    "        else:\n",
    "            last_syllable = ''\n",
    "            last_vowel = ''\n",
    "      \n",
    "    par = lk.split('-')[0]\n",
    "    l = lk.split('-')[1]\n",
    "\n",
    "    strings[par].append([l, last_vowel, last_syllable, last_word])\n",
    "\n",
    "\n",
    "rhymes = defaultdict(list) # РИФМЫ\n",
    "possible_rhymes = defaultdict(list) # ВОЗМОЖНЫЕ РИФМЫ\n",
    "\n",
    "for par_i in strings.keys():\n",
    "    paragraph = strings[par_i]\n",
    "    for i in range(0, len(paragraph)):\n",
    "        for j in range(0, len(paragraph)):\n",
    "            if j != i and paragraph[i][1] != '' and paragraph[j][1] != '':\n",
    "                # точные рифмы\n",
    "                if paragraph[j][1] == paragraph[i][1]:\n",
    "                    rhymes[par_i+'-'+str(paragraph[i][0])].append(paragraph[j][3])\n",
    "                # возможные рифмы\n",
    "                if len(paragraph[i][2]) == 1: \n",
    "                    if paragraph[j][2] in rhymed_vowels[paragraph[i][2]]:\n",
    "                        possible_rhymes[par_i+'-'+str(paragraph[i][0])].append(paragraph[j][3])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ifc0mrbSD29X",
   "metadata": {
    "id": "Ifc0mrbSD29X"
   },
   "source": [
    "#### Количество слогов и определение языка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ItnNJ-nrFbuP",
   "metadata": {
    "id": "ItnNJ-nrFbuP"
   },
   "outputs": [],
   "source": [
    "def syll_lang(line, hangul):\n",
    "    ko = 0\n",
    "    en = 0\n",
    "\n",
    "    syllable_length = 0\n",
    "    words = line.split(' ')\n",
    "    \n",
    "    en_letters = 'qwertyuioplkjhgfdsazxcvbnm'\n",
    "\n",
    "    for word in words:\n",
    "        word = word.strip(' ,.\\\"\\'[]()!?-%:;')\n",
    "        if len(word) > 0:\n",
    "            if word[0].lower() in en_letters:\n",
    "                syllable_length += eti.syllable_count(word)\n",
    "                en = 1\n",
    "            elif word[0] in hangul:\n",
    "                # в корейском слоговое письмо\n",
    "                syllable_length += len(word)\n",
    "                ko = 1\n",
    "\n",
    "    if ko == 0 and en == 0:\n",
    "        lang = 3\n",
    "    if ko == 1 and en == 0:\n",
    "        lang = 0\n",
    "    if ko == 0 and en == 1:\n",
    "        lang = 1\n",
    "    if ko == 1 and en == 1:\n",
    "        lang = 4\n",
    "\n",
    "    return syllable_length, lang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6BOiv4QZq3XD",
   "metadata": {
    "id": "6BOiv4QZq3XD"
   },
   "source": [
    "### Запись в JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e992d7f",
   "metadata": {
    "id": "0e992d7f"
   },
   "outputs": [],
   "source": [
    "meta = song_meta_lyrics[0]\n",
    "lyrics = song_meta_lyrics[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sGMkqVwQpYD1",
   "metadata": {
    "id": "sGMkqVwQpYD1"
   },
   "outputs": [],
   "source": [
    "# КООРДИНИРОВАНИЕ ВСЕХ ФУНКЦИЙ\n",
    "processed_korean = []\n",
    "processed_russian = []\n",
    "\n",
    "for lk in lyrics.keys():\n",
    "    para = lk.split('-')[0]\n",
    "    string = lk.split('-')[1]\n",
    "    para_id = int(para)*1000 + int(string)\n",
    "\n",
    "    # корейский\n",
    "    kor_words = tag_ana(lyrics[lk][1], ld, hd)\n",
    "    syll_length, lang = syll_lang(lyrics[lk][1], hangul)\n",
    "    \n",
    "    rhymed = ''\n",
    "    for rh in rhymes[lk]:\n",
    "        rhymed += rh + ', '\n",
    "\n",
    "    pos_rhymed = ''\n",
    "    for prhm in possible_rhymes[lk]:\n",
    "        if prhm not in rhymes[lk]:\n",
    "            pos_rhymed += prhm + ', '\n",
    "    \n",
    "    for line in strings[para]:\n",
    "        if line[0] == string:\n",
    "            last_word = line[3]\n",
    "            last_vowel = line[2]\n",
    "\n",
    "    kor_sent_meta = {'possibly_rhymed_with':pos_rhymed.strip(', '),\n",
    "                     'rhymed_with':rhymed.strip(', '),\n",
    "                     'last_word':last_word,\n",
    "                     'last_vowel':last_vowel,\n",
    "                     'length_in_syllables':syll_length}\n",
    "    # выравнивание\n",
    "    kor_alignment = {'off_start':0,\n",
    "                     'off_end':len(lyrics[lk][1])-1,\n",
    "                     'para_id':para_id}\n",
    "    \n",
    "    kor_sent = {'text':lyrics[lk][1],\n",
    "                'words':kor_words,\n",
    "                'lang':lang,\n",
    "                'meta':kor_sent_meta,\n",
    "                'para_alignment':kor_alignment}\n",
    "    \n",
    "    processed_korean.append(kor_sent)\n",
    "\n",
    "    # русский\n",
    "    rus_sent = rus_tagger(lyrics[lk][0], {'para_id':para_id})\n",
    "    processed_russian.append(rus_sent)\n",
    "\n",
    "\n",
    "processed_korean.extend(processed_russian)\n",
    "final = {'meta':meta, 'sentences':processed_korean}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jw-kvjsH4Yhx",
   "metadata": {
    "id": "jw-kvjsH4Yhx"
   },
   "outputs": [],
   "source": [
    "# ЗАПИСЬ В КОНЕЧНЫЙ ФАЙЛ\n",
    "with open('Processed ' + file_name, 'w', encoding='utf-8') as out_file:\n",
    "    json.dump(final, out_file, ensure_ascii=False, indent=3)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "7bd0be0f",
    "Ktru55gh8R78",
    "Ifc0mrbSD29X",
    "6BOiv4QZq3XD",
    "SalyawIaLDQs",
    "M5P9sN-K6g9b",
    "J5eZ19K5NS06"
   ],
   "name": "K-pop songs analizer part 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
